<style>

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 2em;
}

.reveal h3 {
  font-size: 1.2em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.small-code pre code {
  font-size: 0.9em;
}

.reveal .smalltext {
  font-size: 0.75em;
}

</style>


Time series shootout: ARIMA vs. LSTM
========================================================
author: Sigrid Keydana, Trivadis
date: 2017/07/10
autosize: true
incremental:false
width: 1400
height: 900


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Forecasting? That's running ARIMA, right?
</h1>


Running ARIMA can be as easy as...
========================================================
class:small-code
incremental:true

&nbsp;

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=16, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE,
                      cache = TRUE)
```

```{r}
library(ggplot2)
library(dplyr)
library(forecast)
library(tidyr)
bold_text_20 <- element_text(face = "bold", size = 20)
bold_text_16 <- element_text(face = "bold", size = 16)
```

```{r, echo=TRUE}
data("AirPassengers")
alldata <- AirPassengers
train <- window(AirPassengers,end=1958.99)
test <- window(AirPassengers, start = 1959)

fit <- auto.arima(train)
autoplot(forecast(fit,h=20))
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


So why would we want to use anything else?
========================================================

&nbsp;

- preconditions/restrictions (stationarity, constant error variance, no level shifts, linear relationships)
- can model any non-linear function with neural networks 
- esp. RNNs (Recurrent Neural Networks) look promising for sequential data, even when it's not about NLP

Let's compare ARIMA and RNNs on a set of synthetic and real-world benchmarks.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Quick recap: ARIMA
========================================================

&nbsp;

- autoregressive part: _AR(p)_:  

$y_{t} = c + \phi_{1}y_{t-1} + \phi_{2}y_{t-2} + \dots + \phi_{p}y_{t-p} + e_{t}$

- differencing: _I(d)_:  number of times differencing has to be applied to obtain a stationary series

- moving average part: _MA(q)_:  

$y_{t} = c + e_t + \theta_{1}e_{t-1} + \theta_{2}e_{t-2} + \dots + \theta_{q}e_{t-q}$

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


The contender: LSTM
========================================================

&nbsp;

Googling the perfect LSTM picture... ;-)

<figure>
<img src="so_many_lstms.png">
<figcaption>
</figcaption>
</figure>


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Why LSTM? Or: Why Gated Recurrent Units, and not plain RNNs?
========================================================

&nbsp;

- How can we preserve relevant information over many timesteps (_vanishing gradient_)?
- how can we not get distracted by all that recent stuff?

&nbsp;

<table>
<tr>
<td><img src="gru1.png"/></td><td><img src="gru2.png" /></td>
</tr>
<tr>
<td style="font-size: 0.7em;">Source: Stanford CS224n, lecture 11<br />http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture11.pdf</td><td></td>
</tr>
</table>

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Just a little bit more complicated: LSTMs
========================================================

&nbsp;

LSTMs add a further gate and redefine how state is transported, but are similar in effect:

<figure>
<img src="grus.png">
<figcaption>
</figcaption>
</figure>

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />





========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Round 1: Linear trend (synthetic dataset 1)
</h1>


The data
========================================================

&nbsp;

```{r}
source("common.R")
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test))
df <- df %>% gather(key = 'type', value = 'value', train:test)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type), size = 2)  

```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


The tasks
========================================================

&nbsp;

- Task 1: One-step-ahead rolling forecast
- Task 2: Multi-step-ahead rolling forecast (n=4)

&nbsp;

Let's get started!

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


One-step-ahead rolling forecast... ARIMA please!
========================================================
class:small-code
incremental:true

&nbsp;

```{r}
fit <- auto.arima(trend_train)
fit

# 1-step-ahead forecast
preds_list <- forecast_rolling(fit, 1, trend_train, trend_test)
test_rmse <- rmse(trend_test, preds_list$predictions)

df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 fitted = c(fit$fitted, rep(NA, length(trend_test))),
                 preds = c(rep(NA, length(trend_train)), preds_list$predictions),
                 lower = c(rep(NA, length(trend_train)), preds_list$lower),
                 upper = c(rep(NA, length(trend_train)), preds_list$upper))
df <- df %>% gather(key = 'type', value = 'value', train:preds)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.1) + ggtitle(paste0("One-step-ahead rolling forecast from ARIMA(4,1,0): Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)

```

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Now, for the LSTM...
========================================================
class:small-code
incremental:true

&nbsp;

Wait - what kind of parameters/configuration are we talking about?

&nbsp;

```{r, results="hide"}
source("common.R")
source("functions.R")

model_exists <- TRUE

lstm_num_timesteps <- 5
batch_size <- 1
epochs <- 500
lstm_units <- 32
model_type <- "model_lstm_simple"
lstm_type <- "stateless"
data_type <- "data_raw"
test_type <- "TREND"

model_name <- build_model_name(model_type, test_type, lstm_type, data_type, epochs)

# get data into "timesteps form"
X_train <- build_X(trend_train, lstm_num_timesteps) 
y_train <- build_y(trend_train, lstm_num_timesteps) 

X_test <- build_X(trend_test, lstm_num_timesteps) 
y_test <- build_y(trend_test, lstm_num_timesteps) 

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]
```

```{r, echo=TRUE}
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    # lstm_units is 32
    # number of timesteps is 5
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features)) %>% 
    layer_dense(units = 1) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = list(X_test, y_test)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = batch_size)
pred_test <- model %>% predict(X_test, batch_size = batch_size)
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

OK! Results please
========================================================
incremental:true

&nbsp;


```{r}

test_rmse <- rmse(tail(trend_test,length(trend_test) - lstm_num_timesteps), pred_test)

df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps), pred_train, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps), pred_test))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + ggtitle(paste0("One-step-ahead rolling forecast from 32-unit LSTM: Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)

```

OOPS...????

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Seems like LSTM does not like extrapolating 
========================================================
incremental:true

&nbsp;

(from the known range of the data)

Because for an in-range test set, it works very well:

&nbsp;

```{r, results="hide"}
model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))

trend_test <- trend_test_inrange
X_test <- build_X(trend_test, lstm_num_timesteps) 
y_test <- build_y(trend_test, lstm_num_timesteps) 
X_test <- reshape_X_3d(X_test)


pred_test <- model %>% predict(X_test, batch_size = batch_size)

test_rmse <- rmse(tail(trend_test,length(trend_test) - lstm_num_timesteps), pred_test)
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps), pred_train, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps), pred_test))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + ggtitle(paste0("One-step-ahead rolling forecast from 32-unit LSTM: Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)



```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

ARIMA is allowed to work with differences...
========================================================
incremental:true

&nbsp;

... shouldn't LSTM be, too?
(It would also eliminate our out-of-range problem.)

We're using 4 instead of 5 timesteps now.

Result:

```{r, results="hide"}
source("common.R")
source("functions.R")

model_exists <- TRUE

lstm_num_timesteps <- 4 #one less
batch_size <- 1
epochs <- 500
lstm_units <- 32
model_type <- "model_lstm_simple"
lstm_type <- "stateless"
data_type <- "data_diffed"
test_type <- "TREND"

model_name <- build_model_name(model_type, test_type, lstm_type, data_type, epochs)
trend_train_diff <- diff(trend_train)
trend_test_diff <- diff(trend_test)

# get data into "timesteps form"
X_train <- build_X(trend_train_diff, lstm_num_timesteps) 
y_train <- build_y(trend_train_diff, lstm_num_timesteps) 

X_test <- build_X(trend_test_diff, lstm_num_timesteps) 
y_test <- build_y(trend_test_diff, lstm_num_timesteps) 

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features)) %>% 
    layer_dense(units = 1) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  
  model %>% summary()
  
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = list(X_test, y_test)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = batch_size)
pred_test <- model %>% predict(X_test, batch_size = batch_size)
pred_train_undiff <- pred_train + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]

df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)

test_rmse <- rmse(tail(trend_test,length(trend_test) - lstm_num_timesteps - 1), pred_test_undiff)
```

```{r}
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + ggtitle(paste0("One-step-ahead rolling forecast from 32-unit LSTM, with differencing: Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)
```



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

We could try to get further improvement by scaling the data
========================================================
incremental:true

&nbsp;

(although in this case the differences are quite small and homogeneous already)

Result:

```{r, results="hide"}
source("common.R")
source("functions.R")

model_exists <- TRUE

lstm_num_timesteps <- 4 #one less
batch_size <- 1
epochs <- 500
lstm_units <- 32
model_type <- "model_lstm_simple"
lstm_type <- "stateless"
data_type <- "data_diffed_scaled"
test_type <- "TREND"

model_name <- build_model_name(model_type, test_type, lstm_type, data_type, epochs)

trend_train_diff <- diff(trend_train)
trend_test_diff <- diff(trend_test)

# normalize
minval <- min(trend_train_diff)
maxval <- max(trend_train_diff)

trend_train_diff <- normalize(trend_train_diff, minval, maxval)
trend_test_diff <- normalize(trend_test_diff, minval, maxval)

X_train <- build_X(trend_train_diff, lstm_num_timesteps) 
y_train <- build_y(trend_train_diff, lstm_num_timesteps) 

X_test <- build_X(trend_test_diff, lstm_num_timesteps) 
y_test <- build_y(trend_test_diff, lstm_num_timesteps) 

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features)) %>% 
    layer_dense(units = 1) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  
  model %>% summary()
  
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = list(X_test, y_test)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train_undiff <- pred_train + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]

df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
test_rmse <- rmse(tail(trend_test,length(trend_test) - lstm_num_timesteps - 1), pred_test_undiff)
```

```{r}
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + ggtitle(paste0("One-step-ahead rolling forecast from 32-unit LSTM, with differencing and scaling: Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_16, axis.title = bold_text_20, axis.text = bold_text_20)
```

Quite an improvement! Seems like this one goes to LSTM ;-)

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Next up: Multi-step-ahead rolling forecast
========================================================
incremental:true
class:small-code

&nbsp;

Let's see ARIMA fist!

```{r}
source("common.R")
source("functions.R")
fit <- auto.arima(trend_train)
fit

# 4-step-ahead forecast
preds_list <- forecast_rolling(fit, 4, trend_train, trend_test)
pred_test <- drop(preds_list$predictions)

df <- data_frame(time_id = 1:20,
                 test = trend_test)
for(i in seq_len(nrow(pred_test))) {
  varname <- paste0("pred_test", i)
  df <- mutate(df, !!varname := c(rep(NA, i-1),
                                  pred_test[i, ],
                                  rep(NA, 17-i)))
}

calc_multiple_rmse <- function(df) {
  m <- as.matrix(df)
  ground_truth <-m[ ,2]
  pred_cols <- m[ , 8:19]
  rowwise_squared_error_sums <- apply(pred_cols, 2, function(col) sum((col - ground_truth)^2, na.rm = TRUE))
  sqrt(sum(rowwise_squared_error_sums)/length(rowwise_squared_error_sums))
}

multiple_rmse <- calc_multiple_rmse(df)

df <- df %>% gather(key = 'type', value = 'value', -time_id)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(colour = type)) + ggtitle(paste0("4-step-ahead rolling forecast from ARIMA: Test RMSE = ", round(multiple_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)

```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


For LSTM, let's now directly use differenced and scaled data
========================================================
class:small-code

&nbsp;

```{r, results="hide"}
source("common.R")
source("functions.R")

model_exists <- TRUE

lstm_num_predictions <- 4
lstm_num_timesteps <- 4 
batch_size <- 1
epochs <- 500
lstm_units <- 32
model_type <- "model_lstm_time_distributed"
lstm_type <- "stateless"
data_type <- "data_diffed_scaled"
test_type <- "TREND"

model_name <- build_model_name(model_type, test_type, lstm_type, data_type, epochs)

trend_train_diff <- diff(trend_train)
trend_test_diff <- diff(trend_test)

# normalize
minval <- min(trend_train_diff)
maxval <- max(trend_train_diff)

trend_train_diff <- normalize(trend_train_diff, minval, maxval)
trend_test_diff <- normalize(trend_test_diff, minval, maxval)

train_matrix <- build_matrix(trend_train_diff, lstm_num_timesteps + lstm_num_predictions) 
test_matrix <- build_matrix(trend_test_diff, lstm_num_timesteps + lstm_num_predictions) 

X_train <- train_matrix[ ,1:4]
y_train <- train_matrix[ ,5:8]

X_test <- test_matrix[ ,1:4]
y_test <- test_matrix[ ,5:8]


# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

y_train <- reshape_X_3d(y_train)
y_test <- reshape_X_3d(y_test)
```

We're using Keras' TimeDistributed layer to get multi-step forecasts.

&nbsp;

```{r, echo=TRUE}
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features),
               return_sequences = TRUE) %>% 
    time_distributed(layer_dense(units = 1)) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = list(X_test, y_test)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}
pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

```


Multi-step forecast from LSTM
========================================================
class:small-code
incremental:true

&nbsp;

```{r, results="hide"}
pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

# undiff
trend_train_add <- trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
trend_train_add_matrix <- build_matrix(trend_train_add, lstm_num_predictions)
pred_train_undiff <- trend_train_add_matrix + pred_train[ , , 1]

trend_test_add <- trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]
trend_test_add_matrix <- build_matrix(trend_test_add, lstm_num_predictions)
pred_test_undiff <- trend_test_add_matrix + pred_test[ , , 1]


df <- data_frame(time_id = 1:20,
                 test = trend_test)
for(i in seq_len(nrow(pred_test))) {
  varname <- paste0("pred_test", i)
  df <- mutate(df, !!varname := c(rep(NA, lstm_num_timesteps+1),
                                  rep(NA, i-1),
                                  pred_test_undiff[i, ],
                                  rep(NA, 12-i)))
}
calc_multiple_rmse <- function(df) {
  m <- as.matrix(df)
  ground_truth <-m[ ,2]
  pred_cols <- m[ , 3:14]
  rowwise_squared_error_sums <- apply(pred_cols, 2, function(col) sum((col - ground_truth)^2, na.rm = TRUE))
  sqrt(sum(rowwise_squared_error_sums)/length(rowwise_squared_error_sums))
}

multiple_rmse <- calc_multiple_rmse(df)

df <- df %>% gather(key = 'type', value = 'value', test:pred_test12)

```


```{r}
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + ggtitle(paste0("4-step-ahead rolling forecast from 32-unit LSTM, with differencing and scaling: Test RMSE = ", round(multiple_rmse, 2))) +   theme(title = bold_text_16, axis.title = bold_text_20, axis.text = bold_text_20)
```


So this one goes to ARIMA. But we haven't really done any hyperparameter tuning for LSTM.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Round 2: Seasonal data (synthetic dataset 2)
</h1>


The data
========================================================

&nbsp;

```{r}
df <- data_frame(time_id = 1:112,
                 train = c(seasonal_train, rep(NA, length(seasonal_test))),
                 test = c(rep(NA, length(seasonal_train)), seasonal_test))
df <- df %>% gather(key = 'train_test', value = 'value', -time_id)
ggplot(df, aes(x = time_id, y = value, color = train_test)) + geom_line()
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


The tasks
========================================================
incremental:true

&nbsp;

As before:

- Task 1: One-step-ahead rolling forecast
- Task 2: Multi-step-ahead rolling forecast (n=6 this time)

&nbsp;


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



ARIMA for seasonal data, one-step-ahead forecast
========================================================
class:small-code
incremental:true

&nbsp;

```{r}
source("common.R")
source("functions.R")

fit <- auto.arima(seasonal_train)
fit

# 1-step-ahead forecast
preds_list <- forecast_rolling(fit, 1, seasonal_train, seasonal_test)

test_rmse <- rmse(seasonal_test, preds_list$predictions)

df <- data_frame(time_id = 1:112,
                 train = c(seasonal_train, rep(NA, length(seasonal_test))),
                 test = c(rep(NA, length(seasonal_train)), seasonal_test),
                 fitted = c(fit$fitted, rep(NA, length(seasonal_test))),
                 preds = c(rep(NA, length(seasonal_train)), preds_list$predictions),
                 lower = c(rep(NA, length(seasonal_train)), preds_list$lower),
                 upper = c(rep(NA, length(seasonal_train)), preds_list$upper))
df <- df %>% gather(key = 'type', value = 'value', train:preds)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.1) + ggtitle(paste0("One-step-ahead rolling forecast from ARIMA: Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

Seasonal data, one-step-ahead forecast: Enter: LSTM
========================================================
incremental:true

&nbsp;

```{r, results="hide"}
source("common.R")
source("functions.R")

model_exists <- TRUE

lstm_num_timesteps <- 6
batch_size <- 1
epochs <- 500
lstm_units <- 32
lstm_type <- "stateless"
data_type <- "data_diffed_scaled"
test_type <- "SEASONAL"
model_type <- "model_lstm_simple"

model_name <- build_model_name(model_type, test_type, lstm_type, data_type, epochs)

seasonal_train_diff <- diff(seasonal_train)
seasonal_test_diff <- diff(seasonal_test)

minval <- min(seasonal_train_diff)
maxval <- max(seasonal_train_diff)

seasonal_train_diff <- normalize(seasonal_train_diff, minval, maxval)
seasonal_test_diff <- normalize(seasonal_test_diff, minval, maxval)

X_train <- build_X(seasonal_train_diff, lstm_num_timesteps) 
y_train <- build_y(seasonal_train_diff, lstm_num_timesteps) 

X_test <- build_X(seasonal_test_diff, lstm_num_timesteps) 
y_test <- build_y(seasonal_test_diff, lstm_num_timesteps) 

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)


num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features)) %>% 
    layer_dense(units = 1) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  
  model %>% summary()
  
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = list(X_test, y_test)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train_undiff <- pred_train + seasonal_train[(lstm_num_timesteps+1):(length(seasonal_train)-1)]
pred_test_undiff <- pred_test + seasonal_test[(lstm_num_timesteps+1):(length(seasonal_test)-1)]

test_rmse <- rmse(tail(seasonal_test,length(seasonal_test) - lstm_num_timesteps-1), pred_test_undiff)
```

```{r}
df <- data_frame(time_id = 1:112,
                 train = c(seasonal_train, rep(NA, length(seasonal_test))),
                 test = c(rep(NA, length(seasonal_train)), seasonal_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(seasonal_test))),
                 pred_test = c(rep(NA, length(seasonal_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + ggtitle(paste0("One-step-ahead rolling forecast from 32-unit LSTM: Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)
```


So this one's for LSTM again.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



ARIMA for seasonal data, multi-step-ahead forecast
========================================================
class:small-code
incremental:true

&nbsp;

```{r}
source("common.R")
fit <- auto.arima(seasonal_train)
fit

# 6-step-ahead forecast
preds_list <- forecast_rolling(fit, 6, seasonal_train, seasonal_test)
pred_test <- drop(preds_list$predictions)
dim(pred_test)

df <- data_frame(time_id = 1:21,
                 test = seasonal_test)
for(i in seq_len(nrow(pred_test))) {
  varname <- paste0("pred_test", i)
  df <- mutate(df, !!varname := c(rep(NA, i-1),
                                  pred_test[i, ],
                                  rep(NA, 16-i)))
}

calc_multiple_rmse <- function(df) {
  m <- as.matrix(df)
  ground_truth <-m[ ,2]
  pred_cols <- m[ , 8:18]
  rowwise_squared_error_sums <- apply(pred_cols, 2, function(col) sum((col - ground_truth)^2, na.rm = TRUE))
  sqrt(sum(rowwise_squared_error_sums)/length(rowwise_squared_error_sums))
}

multiple_rmse <- calc_multiple_rmse(df)


df <- df %>% gather(key = 'type', value = 'value', -time_id)

ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(colour = type)) +
ggtitle(paste0("6-step-ahead rolling forecast from ARIMA: Test RMSE = ", round(multiple_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Seasonal data, multi-step forecast: enter: LSTM
========================================================
incremental:true

&nbsp;

Let's see:

```{r, results="hide"}
source("common.R")
source("functions.R")

model_exists <- TRUE

lstm_num_predictions <- 6
lstm_num_timesteps <- 6
batch_size <- 1
epochs <- 500
lstm_units <- 32
lstm_type <- "stateless"
data_type <- "data_diffed_scaled"
test_type <- "SEASONAL"
model_type <- "model_lstm_time_distributed"

model_name <- build_model_name(model_type, test_type, lstm_type, data_type, epochs)
seasonal_train_diff <- diff(seasonal_train)
seasonal_test_diff <- diff(seasonal_test)

# normalize
minval <- min(seasonal_train_diff)
maxval <- max(seasonal_train_diff)

seasonal_train_diff <- normalize(seasonal_train_diff, minval, maxval)
seasonal_test_diff <- normalize(seasonal_test_diff, minval, maxval)

seasonal_matrix_train <- build_matrix(seasonal_train_diff, lstm_num_timesteps + lstm_num_predictions) 
seasonal_matrix_test <- build_matrix(seasonal_test_diff, lstm_num_timesteps + lstm_num_predictions) 

X_train <- seasonal_matrix_train[ ,1:6]
y_train <- seasonal_matrix_train[ ,7:12]

X_test <- seasonal_matrix_test[ ,1:6]
y_test <- seasonal_matrix_test[ ,7:12]

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

y_train <- reshape_X_3d(y_train)
y_test <- reshape_X_3d(y_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features),
               return_sequences = TRUE) %>% 
    time_distributed(layer_dense(units = 1)) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  
  model %>% summary()
  
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = list(X_test, y_test)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

# undiff
seasonal_train_add <- seasonal_train[(lstm_num_timesteps+1):(length(seasonal_train)-1)]
seasonal_train_add_matrix <- build_matrix(seasonal_train_add, lstm_num_predictions)
pred_train_undiff <- seasonal_train_add_matrix + pred_train[ , , 1]

seasonal_test_add <- seasonal_test[(lstm_num_timesteps+1):(length(seasonal_test)-1)]
seasonal_test_add_matrix <- build_matrix(seasonal_test_add, lstm_num_predictions)
pred_test_undiff <- seasonal_test_add_matrix + pred_test[ , , 1]


df <- data_frame(time_id = 1:21,
                 test = seasonal_test)
for(i in seq_len(nrow(pred_test))) {
  varname <- paste0("pred_test", i)
  df <- mutate(df, !!varname := c(rep(NA, lstm_num_timesteps+1),
                                  rep(NA, i-1),
                                  pred_test_undiff[i, ],
                                  rep(NA, 9-i)))
}
calc_multiple_rmse <- function(df) {
  m <- as.matrix(df)
  ground_truth <-m[ ,2]
  pred_cols <- m[ , 3:11]
  rowwise_squared_error_sums <- apply(pred_cols, 2, function(col) sum((col - ground_truth)^2, na.rm = TRUE))
  sqrt(sum(rowwise_squared_error_sums)/length(rowwise_squared_error_sums))
}

multiple_rmse <- calc_multiple_rmse(df)
df <- df %>% gather(key = 'type', value = 'value', test:pred_test9)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type, linetype=type)) +
ggtitle(paste0("6-step-ahead rolling forecast from 32-unit LSTM: Test RMSE = ", round(multiple_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)


```


Pretty close! 

And again, no hyperparameter tuning involved.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

Intermediate result after synthetic datasets?
========================================================
incremental:true

&nbsp;

ARIMA leading with 2.5 : 1.5

&nbsp;

Time to look at some real data. 

We will now zoom in on one-step-ahead forecasts only, and we will choose two very different datasets: one a classic we are sure ARIMA will handle flawlessly, and one ... well, let's see!

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Round 3: AirPassengers 
</h1>


The "Iris of time series"
========================================================
class:smalltext
incremental:true

&nbsp;

```{r}
data("AirPassengers")

autoplot(AirPassengers) + ggtitle("Air Passengers dataset") +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)
```

Things to note:

- we have a trend
- we have seasonality
- variance is increasing (heteroscedasticity)

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


The task
========================================================

&nbsp;

Just one: One-step-ahead rolling forecast
&nbsp;


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


AirPassengers, enter: ARIMA
========================================================
class:small-code
incremental:true

&nbsp;

```{r}
alldata <- AirPassengers

airp_train <- window(AirPassengers,end=1958.99)
airp_test <- window(AirPassengers, start = 1959)

num_train <- length(airp_train)
num_test <- length(airp_test)
num_all <- num_train + num_test

fit <- auto.arima(airp_train)
fit
preds_list <- forecast_rolling(fit, 1, airp_train, airp_test)

test_rmse <- rmse(airp_test, preds_list$predictions)

df <- data_frame(
                time_id = 1:num_all,
                 train = c(airp_train, rep(NA, length(airp_test))),
                 test = c(rep(NA, length(airp_train)), airp_test),
                 fitted = c(fit$fitted, rep(NA, length(airp_test))),
                 preds = c(rep(NA, length(airp_train)), preds_list$predictions),
                 lower = c(rep(NA, length(airp_train)), preds_list$lower),
                 upper = c(rep(NA, length(airp_train)), preds_list$upper))
df <- df %>% gather(key = 'type', value = 'value', train:preds)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.1) +
ggtitle(paste0("One-step-ahead rolling forecast from ARIMA: Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)

```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


AirPassengers, enter: LSTM
========================================================
class:small-code
incremental:true

&nbsp;

With this small dataset, we are pretty restricted regarding the number of timesteps used.
We're using 12 here:


```{r, results="hide"}
airp_train <- window(AirPassengers,end=1958.99)
airp_test <- window(AirPassengers, start = 1959)

num_train <- length(airp_train)
num_test <- length(airp_test)
num_all <- num_train + num_test


model_exists <- TRUE

lstm_num_timesteps <- 12
batch_size <- 1
epochs <- 500
lstm_units <- 32
model_type <- "model_lstm_simple"
lstm_type <- "stateless"
data_type <- "data_diffed_scaled"
test_type <- "AIRP"

model_name <- build_model_name(model_type, test_type, lstm_type, data_type, epochs)

train_diff <- diff(airp_train)[!is.na(diff(airp_train))]
test_diff <- diff(airp_test)[!is.na(diff(airp_test))]

# normalize
minval <- min(train_diff)
maxval <- max(train_diff)

train_diff <- normalize(train_diff, minval, maxval)
test_diff <- normalize(test_diff, minval, maxval)

X_train <- build_X(train_diff, lstm_num_timesteps) 
y_train <- build_y(train_diff, lstm_num_timesteps) 

X_test <- build_X(test_diff, lstm_num_timesteps) 
y_test <- build_y(test_diff, lstm_num_timesteps) 

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features)) %>% 
    layer_dense(units = 1) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  
  model %>% summary()
  
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = list(X_test, y_test)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train_undiff <- pred_train + airp_train[(lstm_num_timesteps+1):(length(airp_train)-1)]
pred_test_undiff <- pred_test + airp_test[(lstm_num_timesteps+1):(length(airp_test)-1)]

test_rmse <- rmse(tail(airp_test,length(airp_test) - lstm_num_timesteps - 1), pred_test_undiff)

df <- data_frame(
                 time_id = 1:144,
                 train = c(airp_train, rep(NA, length(airp_test))),
                 test = c(rep(NA, length(airp_train)), airp_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(airp_test))),
                 pred_test = c(rep(NA, length(airp_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff)
   )
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) +
ggtitle(paste0("One-step-ahead rolling forecast from 32-unit LSTM: Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)

```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Intermediate result, before final round... Drumroll...
========================================================
class:small-code
incremental:true

&nbsp;

... ARIMA leading with 3.5 : 1.5.

Wait - is this really fair? No experimenting with hyperparameters, and not even with the number of hidden units?

For sure it's not - for example, with 128 hidden units instead of 32, we're already down to RMSE = 19.5.

But let's accept the current standings and look at the final dataset...

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Round 4: Internet traffic data
</h1>


The data
========================================================
class:smalltext

&nbsp;

As per the description... 

"_Internet traffic data (in bits) from a private ISP with centres in 11 European cities. The data corresponds to a transatlantic link and was collected from 06:57 hours on 7 June to 11:17 hours on 31 July 2005. Hourly data._"

```{r,fig.width=20,fig.height=10}
traffic_df <- read_csv("internet-traffic-data-in-bits-fr.csv", col_names = c("hour", "bits"), skip = 1)
ggplot(traffic_df, aes(x = hour, y = bits)) + geom_line() + ggtitle("Air Passengers dataset") +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)

```

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Enter: ARIMA
========================================================
class:smalltext
incremental:true

&nbsp;

Let's do it the auto.arima way as usual...

&nbsp;

```{r, echo=TRUE}
train <- traffic_df$bits[1:800]
test <- traffic_df$bits[801:nrow(traffic_df)]

fit <- auto.arima(train)
fit
```

&nbsp;

... hm???

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


ARIMA: 2nd try
========================================================
class:smalltext
incremental:true

&nbsp;

Let's give ARIMA some information...

&nbsp;

```{r, echo=TRUE}
train_ts <- msts(train,seasonal.periods = c(24, 24*7))
fit <- auto.arima(train_ts)
fit
```

&nbsp;

Still not what we wanted.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

ARIMA: 3nd try
========================================================
class:smalltext
incremental:true

&nbsp;

Perhaps an exhaustive search - over all models up to a specific capacity - will make things better?

&nbsp;

```{r, echo=TRUE}
#fit <- auto.arima(train_ts, stepwise = FALSE)
```

&nbsp;

Honestly, I did not have the patience to wait for this to finish... but it didn't look good ;-)

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

Anything else we could do?



Trying TBATS
========================================================
class:smalltext
incremental:true
class:smallcode

&nbsp;

TBATS (_Exponential smoothing state space model with Box-Cox transformation, ARMA errors, Trend and Seasonal components_) is a method designed to cope with complex seasonal patterns. 


&nbsp;

```{r, echo=TRUE}
fit <- tbats(train_ts)
fit
```

&nbsp;


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

Predictions from TBATS 
========================================================
class:smalltext
incremental:true

&nbsp;

```{r}
preds_list <- readRDS("preds_list.rds")
test_rmse <- rmse(test, preds_list$predictions)

df <- data_frame(
                time_id = 1:1231,
                 train_ = c(train, rep(NA, length(test))),
                 test_ = c(rep(NA, length(train)), test),
                 fitted = c(fit$fitted, rep(NA, length(test))),
                 preds = c(rep(NA, length(train)), preds_list$predictions),
                 lower = c(rep(NA, length(train)), preds_list$lower),
                 upper = c(rep(NA, length(train)), preds_list$upper))
df <- df %>% gather(key = 'type', value = 'value', train_:preds)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.1) +
ggtitle(paste0("One-step-ahead rolling forecast from TBATS: Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)

```

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Time to LSTM...!
========================================================
class:smalltext
incremental:true

&nbsp;



&nbsp;

```{r, results="hide"}
traffic_df <- read_csv("internet-traffic-data-in-bits-fr.csv", col_names = c("hour", "bits"), skip = 1)

internet_train <- traffic_df$bits[1:800]
internet_test <- traffic_df$bits[801:nrow(traffic_df)]

model_exists <- TRUE

lstm_num_timesteps <- 7*24
batch_size <- 1
epochs <- 500
lstm_units <- 32
model_type <- "model_lstm_simple"
lstm_type <- "stateless"
data_type <- "data_diffed_scaled"
test_type <- "INTERNET"

model_name <- build_model_name(model_type, test_type, lstm_type, data_type, epochs)

train_diff <- diff(internet_train)[!is.na(diff(internet_train))]
test_diff <- diff(internet_test)[!is.na(diff(internet_test))]

# normalize
minval <- min(train_diff)
maxval <- max(train_diff)

train_diff <- normalize(train_diff, minval, maxval)
test_diff <- normalize(test_diff, minval, maxval)

X_train <- build_X(train_diff, lstm_num_timesteps) 
y_train <- build_y(train_diff, lstm_num_timesteps) 

X_test <- build_X(test_diff, lstm_num_timesteps) 
y_test <- build_y(test_diff, lstm_num_timesteps) 

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features)) %>% 
    layer_dense(units = 1) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam',
      callbacks = callback_early_stopping(patience=2)
    )
  
  model %>% summary()
  
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = list(X_test, y_test)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train_undiff <- pred_train + internet_train[(lstm_num_timesteps+1):(length(internet_train)-1)]
pred_test_undiff <- pred_test + internet_test[(lstm_num_timesteps+1):(length(internet_test)-1)]

test_rmse <- rmse(tail(internet_test,length(internet_test) - lstm_num_timesteps - 1), pred_test_undiff)

df <- data_frame(
                 time_id = 1:1231,
                 train = c(internet_train, rep(NA, length(internet_test))),
                 test = c(rep(NA, length(internet_train)), internet_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(internet_test))),
                 pred_test = c(rep(NA, length(internet_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff)
   )
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)

ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))  +
ggtitle(paste0("One-step-ahead rolling forecast from 32-unit LSTM: Test RMSE = ", round(test_rmse, 2))) +   theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)


```

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

